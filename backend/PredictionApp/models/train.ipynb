{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import torchtext.data\n",
    "from torchtext import vocab\n",
    "from torchtext.vocab import Vocab\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "spacy_processor = spacy.load('en_core_web_sm')\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  101513\n"
     ]
    }
   ],
   "source": [
    "# Load IMDB data\n",
    "TEXTS = torchtext.data.Field(\n",
    "    lower = True,\n",
    "    tokenize = 'spacy',\n",
    "    tokenizer_language = 'en_core_web_sm'\n",
    ")\n",
    "\n",
    "LABELS = torchtext.data.LabelField(dtype = torch.float)\n",
    "\n",
    "# split data\n",
    "train, _ = torchtext.datasets.IMDB.splits(\n",
    "    text_field = TEXTS,\n",
    "    label_field = LABELS,\n",
    "    train = 'train',\n",
    "    test = 'test',\n",
    "    path = 'aclImdb'\n",
    ")\n",
    "\n",
    "# Load GloVe vectors\n",
    "loaded_vectors = torchtext.vocab.Vectors('glove.6B.50d.txt') #loaded_vectors = vocab.GloVe(name = '6B', dim = 50)\n",
    "\n",
    "# Build vocabulary based on training data\n",
    "TEXTS.build_vocab(train, vectors = loaded_vectors, max_size = len(loaded_vectors.stoi))\n",
    "# Assing vectors to vocabulary tokens\n",
    "TEXTS.vocab.set_vectors(stoi = loaded_vectors.stoi, vectors = loaded_vectors.vectors, dim = loaded_vectors.dim)\n",
    "# Build label vocabulary\n",
    "LABELS.build_vocab(train)\n",
    "\n",
    "# print vocabulary size\n",
    "print(f'Vocabulary size: {len(TEXTS.vocab)}')\n",
    "\n",
    "# save tokenizer\n",
    "torch.save(TEXTS, 'torchtext_tokenizer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torch.load('torchtext_tokenizer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embedding): Embedding(101982, 50, padding_idx=1)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 50), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 50), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 50), stride=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n_filters, kernel_size = (fs, embedding_dim)) \n",
    "            for fs\n",
    "            in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text) # (n, T) -> (n, T, E)\n",
    "        embedded = embedded.unsqueeze(1) # (n, T, E) -> (n, 1, T, E)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs] # (n, 1, T, E) -> (n, n_filters, T - d_filter + 1)\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved] # (n, n_filters, T - d_filter + 1) -> (n, n_filters)\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1)) # (n, n_filters) -> (n, n_filters * len(filter_sizes))\n",
    "        logit = self.fc(cat)\n",
    "        return logit\n",
    "    \n",
    "model = torch.load('imdb-model-cnn.pt')\n",
    "model.eval()\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spacy\n",
    "## Torch + Model\n",
    "## Torchtext + TEXTS.vocab.stoi (torchtext.data.Field)\n",
    "## Captum + LayerIntegratedGradients + TokenReferenceBase\n",
    "\n",
    "def analyse_string(model, sentence, min_len = 7):\n",
    "    # reference/baseline token index = pad token index\n",
    "    pad_index = tokenizer.vocab.stoi['pad']\n",
    "    token_reference = TokenReferenceBase(reference_token_idx = pad_index)\n",
    "    # initialize feature attribution model\n",
    "    lig = LayerIntegratedGradients(model, model.embedding)\n",
    "    # tokenize text\n",
    "    text = [token.text for token in spacy_processor.tokenizer(sentence)]\n",
    "    # pad to minimum length\n",
    "    if len(text) < min_len:\n",
    "        text += ['pad'] * (min_len - len(text))\n",
    "    # get indices for token strings\n",
    "    indices = [tokenizer.vocab.stoi[token] for token in text]\n",
    "    # clear gradients\n",
    "    model.zero_grad()\n",
    "    # initialize input tensor\n",
    "    indices_tensor = torch.tensor(indices, device = DEVICE)\n",
    "    # induce batch dim\n",
    "    indices_tensor = indices_tensor.unsqueeze(0)\n",
    "    # save input seuqnce length\n",
    "    seq_length = len(text) #min_len\n",
    "    # predict probability with model\n",
    "    pred_prob = torch.sigmoid(model(indices_tensor)).item()\n",
    "    # generate reference indices\n",
    "    reference_indices = token_reference.generate_reference(seq_length, device = DEVICE).unsqueeze(0)\n",
    "    # compute attributions using layer-integrated gradients\n",
    "    attributions, delta = lig.attribute(\n",
    "        indices_tensor,\n",
    "        reference_indices,\n",
    "        n_steps = 500,\n",
    "        return_convergence_delta = True\n",
    "    )\n",
    "    # sum attributions along embedding dimensions and norm to [-1, 1]\n",
    "    attributions = attributions.sum(dim = 2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    # return probability, attributions and text\n",
    "    return pred_prob, attributions, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998109936714172 \n",
      " [ 0.01607962 -0.10480973 -0.37624715 -0.37292588  0.51327613  0.32549392\n",
      "  0.5588112  -0.16264103] \n",
      " ['It', 'was', 'a', 'truly', 'fantastic', 'performance', 'today', '!']\n"
     ]
    }
   ],
   "source": [
    "prob, attrs, text = analyse_string(model, 'It was a truly fantastic performance today!')\n",
    "\n",
    "print(prob, '\\n', attrs, '\\n', text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
